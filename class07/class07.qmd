---
title: "Class 07: Machine Learning 1"
author: "Olivia Baldwin"
format: pdf
---

Before we get into clustering methods let's make some sample data to cluster where we know what it should look like. 

To help, I will use the `rnorm()` function. 

```{r}
hist(c(rnorm(15000, mean=-3), rnorm(15000, mean=3)))

```


```{r}
n=30
x <- c(rnorm(n, mean=3), rnorm(n, mean=-3))
y <- rev(x)

z <- cbind(x,y)

#cbind = by column
#rbind = by row

plot(z)
```


##K-means clustering 
The function in base R for k-means clustering is called `kmeans()`. 
  - argument "x" = input and "centers" = how many clusers you want
  - "clusering vector" tells me which cluster each data point belongs to
  -"sum of squares" tells me how far each point in cluster are in the center/how tightly grouped (good) the cluster is

```{r}
help(kmeans)
```

```{r}
km <- kmeans(z, centers=2)
km
```

Can get to the "components" by printing them out like you would columns (usuing a "$"). 

```{r}
#examples of printing components

km$cluster
km$centers
```


Plot Z with the clusters colored and add cluster centers: 

```{r}
plot(z, col= km$cluster)
points(km$centers, col="blue", pch=15, cex=2)

#pch changed the point type to the squares
#cex expanded the characters to size 2 instead of 1
```


Can you cluster our data in `z` into four clusters? 

```{r}
km4 <- kmeans(z, centers=4)
```

```{r}
plot(z, col= km4$cluster)
points(km4$centers, col="blue", pch=15, cex=2)

#this clustering has no basis, usually use a "scree plot" to look for an inflection point to hint towards the most probably number of clusters (K) 
```


##Hierarchical Clustering

The main function for hierarchical clustering is `hclust()`.
  - can do top down or bottom up 

Unlike `kmeans()` I cannot just pass in my data as an input, I first need a distrance matrix from my data. 

```{r}
d <-dist(z)
hc <- hclust(d)
hc
```

There is a specific hclust plot() method... 

```{r}
plot(hc)
abline(h=10, col="red")

#the line represents where you would "cut" the branches
#cut branches in the "easiest" spot, meaning the tallest height to determine clusters
```

To get my clustering result (i.e. the membership vector) I can "cut" the dendrogram at a given height. To do this I will use the `cutree()` function. 

```{r}
groups <- cutree(hc, h=10)
plot(z, col=groups) 
```


# Principal Component Analysis 

"Principal component analysis (PCA) is a well established "multivariate statistical technique" used to reduce the dimensionality of a complex data set to a more manageable number (typically 2D or 3D). This method is particularly useful for highlighting strong paterns and relationships in large datasets (i.e. revealing major similarities and diferences) that are otherwise hard to visualize."


## PCA for UK foods

```{r}
url <- "https://tinyurl.com/UK-foods"
uk <- read.csv(url)
```

>Q1: 17 rows and 5 columns

```{r}
dim(uk)
head(uk)
```


```{r}
uk <- read.csv(url, row.names=1)
uk
```


>Q2: I like the alternate option better to set the row names when you import so you don't have to override the variable. 

Barplot: 
```{r}
barplot(as.matrix(uk), beside=T, col=rainbow(nrow(uk)))
```

>Q3: Changing the beside argument will change the barplot. 

```{r}
barplot(as.matrix(uk), beside=FALSE, col=rainbow(nrow(uk)))
```

>Q5: each y axis is aligned to the country in its row and being on the diagonal means the other country on each x matches the country on the y.

```{r}
pairs(uk, col=rainbow(10), pch=16)
```

>Q6: There are several points that are very different in the N.Ireland row (i.e. light blue, yellow, green, and orange that are higher on the x axis)

##PCA to the rescue

The main function to do PCA in base R is `prcomp()`.

Note that I need to take the transpose of this particular data as that is what the `prcomp()` help page asked for. 

```{r}
pca <- prcomp( t(uk) )
summary(pca)
```

Lets see what is inside our result object `pca` that we just calculated. 

```{r}
attributes(pca)
```

```{r}
pca$x
```

To make our main result figure, called a "PC plot" (or score plot or ordination plot or PC1 vs. PC2). 

```{r}
plot(pca$x[,1], pca$x[,2], col=c("black", "red", "blue", "green"), pch=16, xlab="PC1 (67.4%)", ylab="PC2(29%)")

#this will access the first and second column of the PC table output (a.k.a. PC1 and PC2)
```

## Variable Loadings Plot

This plot can show how these countries differ from each other. The positive means it is moving towards the positive end of PCA1. 

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

